{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1MkY8NTXkhRgKNmIyAanAyCIWGiDrBbeE","authorship_tag":"ABX9TyMLDR18IWu1ifZpmzdLnRYC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GGDVs1Q4SYV0","executionInfo":{"status":"ok","timestamp":1669474175798,"user_tz":-480,"elapsed":4396,"user":{"displayName":"胡博程","userId":"10006746442944693195"}},"outputId":"d62d19cc-7fb3-444a-be9b-c0957d219289"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import time\n","import torch\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","from importlib import import_module\n","import argparse\n"],"metadata":{"id":"cBkiy9j1S5Hk","executionInfo":{"status":"ok","timestamp":1669474180188,"user_tz":-480,"elapsed":2543,"user":{"displayName":"胡博程","userId":"10006746442944693195"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["bert init模块"],"metadata":{"id":"NGtJkyK8WFPs"}},{"cell_type":"code","source":["# coding=utf-8\n","# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\"\"\"PyTorch optimization for BERT model.\"\"\"\n","\n","import math\n","import torch\n","from torch.optim import Optimizer\n","from torch.optim.optimizer import required\n","from torch.nn.utils import clip_grad_norm_\n","import logging\n","import abc\n","import sys\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","if sys.version_info >= (3, 4):\n","    ABC = abc.ABC\n","else:\n","    ABC = abc.ABCMeta('ABC', (), {})\n","\n","\n","class _LRSchedule(ABC):\n","    \"\"\" Parent of all LRSchedules here. \"\"\"\n","    warn_t_total = False        # is set to True for schedules where progressing beyond t_total steps doesn't make sense\n","    def __init__(self, warmup=0.002, t_total=-1, **kw):\n","        \"\"\"\n","        :param warmup:  what fraction of t_total steps will be used for linear warmup\n","        :param t_total: how many training steps (updates) are planned\n","        :param kw:\n","        \"\"\"\n","        super(_LRSchedule, self).__init__(**kw)\n","        if t_total < 0:\n","            logger.warning(\"t_total value of {} results in schedule not being applied\".format(t_total))\n","        if not 0.0 <= warmup < 1.0 and not warmup == -1:\n","            raise ValueError(\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\".format(warmup))\n","        warmup = max(warmup, 0.)\n","        self.warmup, self.t_total = float(warmup), float(t_total)\n","        self.warned_for_t_total_at_progress = -1\n","\n","    def get_lr(self, step, nowarn=False):\n","        \"\"\"\n","        :param step:    which of t_total steps we're on\n","        :param nowarn:  set to True to suppress warning regarding training beyond specified 't_total' steps\n","        :return:        learning rate multiplier for current update\n","        \"\"\"\n","        if self.t_total < 0:\n","            return 1.\n","        progress = float(step) / self.t_total\n","        ret = self.get_lr_(progress)\n","        # warning for exceeding t_total (only active with warmup_linear\n","        if not nowarn and self.warn_t_total and progress > 1. and progress > self.warned_for_t_total_at_progress:\n","            logger.warning(\n","                \"Training beyond specified 't_total'. Learning rate multiplier set to {}. Please set 't_total' of {} correctly.\"\n","                    .format(ret, self.__class__.__name__))\n","            self.warned_for_t_total_at_progress = progress\n","        # end warning\n","        return ret\n","\n","    @abc.abstractmethod\n","    def get_lr_(self, progress):\n","        \"\"\"\n","        :param progress:    value between 0 and 1 (unless going beyond t_total steps) specifying training progress\n","        :return:            learning rate multiplier for current update\n","        \"\"\"\n","        return 1.\n","\n","\n","class ConstantLR(_LRSchedule):\n","    def get_lr_(self, progress):\n","        return 1.\n","\n","\n","class WarmupCosineSchedule(_LRSchedule):\n","    \"\"\"\n","    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n","    Decreases learning rate from 1. to 0. over remaining `1 - warmup` steps following a cosine curve.\n","    If `cycles` (default=0.5) is different from default, learning rate follows cosine function after warmup.\n","    \"\"\"\n","    warn_t_total = True\n","    def __init__(self, warmup=0.002, t_total=-1, cycles=.5, **kw):\n","        \"\"\"\n","        :param warmup:      see LRSchedule\n","        :param t_total:     see LRSchedule\n","        :param cycles:      number of cycles. Default: 0.5, corresponding to cosine decay from 1. at progress==warmup and 0 at progress==1.\n","        :param kw:\n","        \"\"\"\n","        super(WarmupCosineSchedule, self).__init__(warmup=warmup, t_total=t_total, **kw)\n","        self.cycles = cycles\n","\n","    def get_lr_(self, progress):\n","        if progress < self.warmup:\n","            return progress / self.warmup\n","        else:\n","            progress = (progress - self.warmup) / (1 - self.warmup)   # progress after warmup\n","            return 0.5 * (1. + math.cos(math.pi * self.cycles * 2 * progress))\n","\n","\n","class WarmupCosineWithHardRestartsSchedule(WarmupCosineSchedule):\n","    \"\"\"\n","    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n","    If `cycles` (default=1.) is different from default, learning rate follows `cycles` times a cosine decaying\n","    learning rate (with hard restarts).\n","    \"\"\"\n","    def __init__(self, warmup=0.002, t_total=-1, cycles=1., **kw):\n","        super(WarmupCosineWithHardRestartsSchedule, self).__init__(warmup=warmup, t_total=t_total, cycles=cycles, **kw)\n","        assert(cycles >= 1.)\n","\n","    def get_lr_(self, progress):\n","        if progress < self.warmup:\n","            return progress / self.warmup\n","        else:\n","            progress = (progress - self.warmup) / (1 - self.warmup)     # progress after warmup\n","            ret = 0.5 * (1. + math.cos(math.pi * ((self.cycles * progress) % 1)))\n","            return ret\n","\n","\n","class WarmupCosineWithWarmupRestartsSchedule(WarmupCosineWithHardRestartsSchedule):\n","    \"\"\"\n","    All training progress is divided in `cycles` (default=1.) parts of equal length.\n","    Every part follows a schedule with the first `warmup` fraction of the training steps linearly increasing from 0. to 1.,\n","    followed by a learning rate decreasing from 1. to 0. following a cosine curve.\n","    \"\"\"\n","    def __init__(self, warmup=0.002, t_total=-1, cycles=1., **kw):\n","        assert(warmup * cycles < 1.)\n","        warmup = warmup * cycles if warmup >= 0 else warmup\n","        super(WarmupCosineWithWarmupRestartsSchedule, self).__init__(warmup=warmup, t_total=t_total, cycles=cycles, **kw)\n","\n","    def get_lr_(self, progress):\n","        progress = progress * self.cycles % 1.\n","        if progress < self.warmup:\n","            return progress / self.warmup\n","        else:\n","            progress = (progress - self.warmup) / (1 - self.warmup)     # progress after warmup\n","            ret = 0.5 * (1. + math.cos(math.pi * progress))\n","            return ret\n","\n","\n","class WarmupConstantSchedule(_LRSchedule):\n","    \"\"\"\n","    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n","    Keeps learning rate equal to 1. after warmup.\n","    \"\"\"\n","    def get_lr_(self, progress):\n","        if progress < self.warmup:\n","            return progress / self.warmup\n","        return 1.\n","\n","\n","class WarmupLinearSchedule(_LRSchedule):\n","    \"\"\"\n","    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n","    Linearly decreases learning rate from 1. to 0. over remaining `1 - warmup` steps.\n","    \"\"\"\n","    warn_t_total = True\n","    def get_lr_(self, progress):\n","        if progress < self.warmup:\n","            return progress / self.warmup\n","        return max((progress - 1.) / (self.warmup - 1.), 0.)\n","\n","\n","SCHEDULES = {\n","    None:       ConstantLR,\n","    \"none\":     ConstantLR,\n","    \"warmup_cosine\": WarmupCosineSchedule,\n","    \"warmup_constant\": WarmupConstantSchedule,\n","    \"warmup_linear\": WarmupLinearSchedule\n","}\n","\n","\n","class BertAdam(Optimizer):\n","    \"\"\"Implements BERT version of Adam algorithm with weight decay fix.\n","    Params:\n","        lr: learning rate\n","        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\n","        t_total: total number of training steps for the learning\n","            rate schedule, -1  means constant learning rate of 1. (no warmup regardless of warmup setting). Default: -1\n","        schedule: schedule to use for the warmup (see above).\n","            Can be `'warmup_linear'`, `'warmup_constant'`, `'warmup_cosine'`, `'none'`, `None` or a `_LRSchedule` object (see below).\n","            If `None` or `'none'`, learning rate is always kept constant.\n","            Default : `'warmup_linear'`\n","        b1: Adams b1. Default: 0.9\n","        b2: Adams b2. Default: 0.999\n","        e: Adams epsilon. Default: 1e-6\n","        weight_decay: Weight decay. Default: 0.01\n","        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\n","    \"\"\"\n","    def __init__(self, params, lr=required, warmup=-1, t_total=-1, schedule='warmup_linear',\n","                 b1=0.9, b2=0.999, e=1e-6, weight_decay=0.01, max_grad_norm=1.0, **kwargs):\n","        if lr is not required and lr < 0.0:\n","            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n","        if not isinstance(schedule, _LRSchedule) and schedule not in SCHEDULES:\n","            raise ValueError(\"Invalid schedule parameter: {}\".format(schedule))\n","        if not 0.0 <= b1 < 1.0:\n","            raise ValueError(\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\".format(b1))\n","        if not 0.0 <= b2 < 1.0:\n","            raise ValueError(\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\".format(b2))\n","        if not e >= 0.0:\n","            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(e))\n","        # initialize schedule object\n","        if not isinstance(schedule, _LRSchedule):\n","            schedule_type = SCHEDULES[schedule]\n","            schedule = schedule_type(warmup=warmup, t_total=t_total)\n","        else:\n","            if warmup != -1 or t_total != -1:\n","                logger.warning(\"warmup and t_total on the optimizer are ineffective when _LRSchedule object is provided as schedule. \"\n","                               \"Please specify custom warmup and t_total in _LRSchedule object.\")\n","        defaults = dict(lr=lr, schedule=schedule,\n","                        b1=b1, b2=b2, e=e, weight_decay=weight_decay,\n","                        max_grad_norm=max_grad_norm)\n","        super(BertAdam, self).__init__(params, defaults)\n","\n","    def get_lr(self):\n","        lr = []\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                state = self.state[p]\n","                if len(state) == 0:\n","                    return [0]\n","                lr_scheduled = group['lr']\n","                lr_scheduled *= group['schedule'].get_lr(state['step'])\n","                lr.append(lr_scheduled)\n","        return lr\n","\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['next_m'] = torch.zeros_like(p.data)\n","                    # Exponential moving average of squared gradient values\n","                    state['next_v'] = torch.zeros_like(p.data)\n","\n","                next_m, next_v = state['next_m'], state['next_v']\n","                beta1, beta2 = group['b1'], group['b2']\n","\n","                # Add grad clipping\n","                if group['max_grad_norm'] > 0:\n","                    clip_grad_norm_(p, group['max_grad_norm'])\n","\n","                # Decay the first and second moment running average coefficient\n","                # In-place operations to update the averages at the same time\n","                next_m.mul_(beta1).add_(1 - beta1, grad)\n","                next_v.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                update = next_m / (next_v.sqrt() + group['e'])\n","\n","                # Just adding the square of the weights to the loss function is *not*\n","                # the correct way of using L2 regularization/weight decay with Adam,\n","                # since that will interact with the m and v parameters in strange ways.\n","                #\n","                # Instead we want to decay the weights in a manner that doesn't interact\n","                # with the m/v parameters. This is equivalent to adding the square\n","                # of the weights to the loss with plain (non-momentum) SGD.\n","                if group['weight_decay'] > 0.0:\n","                    update += group['weight_decay'] * p.data\n","\n","                lr_scheduled = group['lr']\n","                lr_scheduled *= group['schedule'].get_lr(state['step'])\n","\n","                update_with_lr = lr_scheduled * update\n","                p.data.add_(-update_with_lr)\n","\n","                state['step'] += 1\n","\n","                # step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n","                # No bias correction\n","                # bias_correction1 = 1 - beta1 ** state['step']\n","                # bias_correction2 = 1 - beta2 ** state['step']\n","\n","        return loss\n"],"metadata":{"id":"bSjAcXRETF-3","executionInfo":{"status":"ok","timestamp":1669474180188,"user_tz":-480,"elapsed":2,"user":{"displayName":"胡博程","userId":"10006746442944693195"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["utilis模块"],"metadata":{"id":"FkM4dFHjUHxH"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","from tqdm import tqdm\n","import time\n","from datetime import timedelta\n","\n","\n","PAD, CLS = '[PAD]', '[CLS]'  # padding符号, bert中综合信息符号\n","\n","def build_dataset(x_train,y_train,x_dev,y_dev,x_test,y_test,config):\n","\n","    def load_dataset(x, y, pad_size=32):\n","        contents = []\n","        list1 = np.array(x)\n","        list2 = np.array(y)\n","        for i in range(list1.shape[0]):\n","            content, label = list1[i][0], list2[i]  # content放文本，label放0/1\n","            token = config.tokenizer.tokenize(content)\n","            token = [CLS] + token\n","            seq_len = len(token)\n","            mask = []\n","            token_ids = config.tokenizer.convert_tokens_to_ids(token)\n","\n","            if pad_size:\n","                if len(token) < pad_size:\n","                    mask = [1] * len(token_ids) + [0] * (pad_size - len(token))\n","                    token_ids += ([0] * (pad_size - len(token)))\n","                else:\n","                    mask = [1] * pad_size\n","                    token_ids = token_ids[:pad_size]\n","                    seq_len = pad_size\n","            contents.append((token_ids, int(label), seq_len, mask))\n","        return contents\n","\n","    train = load_dataset(x_train, y_train, config.pad_size)\n","    dev = load_dataset(x_dev, y_dev, config.pad_size)\n","    test = load_dataset(x_test, y_test, config.pad_size)\n","    return train, dev, test\n","\n","\n","class DatasetIterater(object):\n","    def __init__(self, batches, batch_size, device):\n","        self.batch_size = batch_size\n","        self.batches = batches\n","        self.n_batches = len(batches) // batch_size\n","        self.residue = False  # 记录batch数量是否为整数\n","        if len(batches) % self.n_batches != 0:\n","            self.residue = True\n","        self.index = 0\n","        self.device = device\n","\n","    def _to_tensor(self, datas):\n","        x = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n","        y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n","\n","        # pad前的长度(超过pad_size的设为pad_size)\n","        seq_len = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n","        mask = torch.LongTensor([_[3] for _ in datas]).to(self.device)\n","        return (x, seq_len, mask), y\n","\n","    def __next__(self):\n","        if self.residue and self.index == self.n_batches:\n","            batches = self.batches[self.index * self.batch_size: len(self.batches)]\n","            self.index += 1\n","            batches = self._to_tensor(batches)\n","            return batches\n","\n","        elif self.index >= self.n_batches:\n","            self.index = 0\n","            raise StopIteration\n","        else:\n","            batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n","            self.index += 1\n","            batches = self._to_tensor(batches)\n","            return batches\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __len__(self):\n","        if self.residue:\n","            return self.n_batches + 1\n","        else:\n","            return self.n_batches\n","\n","\n","def build_iterator(dataset, config):\n","    iter = DatasetIterater(dataset, config.batch_size, config.device)\n","    return iter\n","\n","\n","def get_time_dif(start_time):\n","    \"\"\"获取已使用时间\"\"\"\n","    end_time = time.time()\n","    time_dif = end_time - start_time\n","    return timedelta(seconds=int(round(time_dif)))\n"],"metadata":{"id":"i3Xh_WOEUP0T","executionInfo":{"status":"ok","timestamp":1669474184779,"user_tz":-480,"elapsed":2,"user":{"displayName":"胡博程","userId":"10006746442944693195"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["train_eval模块"],"metadata":{"id":"1v_AxYKpTGuL"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn import metrics\n","import time\n","\n","\n","\n","# 权重初始化，默认xavier\n","def init_network(model, method='xavier', exclude='embedding', seed=123):\n","    for name, w in model.named_parameters():\n","        if exclude not in name:\n","            if len(w.size()) < 2:\n","                continue\n","            if 'weight' in name:\n","                if method == 'xavier':\n","                    nn.init.xavier_normal_(w)\n","                elif method == 'kaiming':\n","                    nn.init.kaiming_normal_(w)\n","                else:\n","                    nn.init.normal_(w)\n","            elif 'bias' in name:\n","                nn.init.constant_(w, 0)\n","            else:\n","                pass\n","\n","\n","def train(config, model, train_iter, dev_iter, test_iter):\n","    start_time = time.time()\n","    model.train()\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n","    # optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n","    optimizer = BertAdam(optimizer_grouped_parameters,\n","                         lr=config.learning_rate,\n","                         warmup=0.05,\n","                         t_total=len(train_iter) * config.num_epochs)\n","    total_batch = 0  # 记录进行到多少batch\n","    dev_best_loss = float('inf')\n","    last_improve = 0  # 记录上次验证集loss下降的batch数\n","    flag = False  # 记录是否很久没有效果提升\n","    model.train()\n","    for epoch in range(config.num_epochs):\n","        print('Epoch [{}/{}]'.format(epoch + 1, config.num_epochs))\n","        for i, (trains, labels) in enumerate(train_iter):\n","            outputs = model(trains)\n","            model.zero_grad()\n","            loss = F.cross_entropy(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            if total_batch % 100 == 0:\n","                # 每多少轮输出在训练集和验证集上的效果\n","                true = labels.data.cpu()\n","                predic = torch.max(outputs.data, 1)[1].cpu()\n","                train_acc = metrics.accuracy_score(true, predic)\n","                dev_acc, dev_loss = evaluate(config, model, dev_iter)\n","                if dev_loss < dev_best_loss:\n","                    dev_best_loss = dev_loss\n","                    torch.save(model.state_dict(), config.save_path)\n","                    improve = '*'\n","                    last_improve = total_batch\n","                else:\n","                    improve = ''\n","                time_dif = get_time_dif(start_time)\n","                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n","                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))\n","                model.train()\n","            total_batch += 1\n","            if total_batch - last_improve > config.require_improvement:\n","                # 验证集loss超过1000batch没下降，结束训练\n","                print(\"No optimization for a long time, auto-stopping...\")\n","                flag = True\n","                break\n","        if flag:\n","            break\n","    test(config, model, test_iter)\n","\n","\n","def test(config, model, test_iter):\n","    # test\n","    model.load_state_dict(torch.load(config.save_path))\n","    model.eval()\n","    start_time = time.time()\n","    test_acc, test_loss, test_report, test_confusion = evaluate(config, model, test_iter, test=True)\n","    msg = 'Test Loss: {0:>5.2},  Test Acc: {1:>6.2%}'\n","    print(msg.format(test_loss, test_acc))\n","    print(\"Precision, Recall and F1-Score...\")\n","    print(test_report)\n","    print(\"Confusion Matrix...\")\n","    print(test_confusion)\n","    time_dif = get_time_dif(start_time)\n","    print(\"Time usage:\", time_dif)\n","\n","\n","def evaluate(config, model, data_iter, test=False):\n","    model.eval()\n","    loss_total = 0\n","    predict_all = np.array([], dtype=int)\n","    labels_all = np.array([], dtype=int)\n","    with torch.no_grad():\n","        for texts, labels in data_iter:\n","            outputs = model(texts)\n","            loss = F.cross_entropy(outputs, labels)\n","            loss_total += loss\n","            labels = labels.data.cpu().numpy()\n","            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n","            labels_all = np.append(labels_all, labels)\n","            predict_all = np.append(predict_all, predic)\n","\n","    acc = metrics.accuracy_score(labels_all, predict_all)\n","    if test:\n","        report = metrics.classification_report(labels_all, predict_all, target_names=config.class_list, digits=4)\n","        confusion = metrics.confusion_matrix(labels_all, predict_all)\n","        return acc, loss_total / len(data_iter), report, confusion\n","    return acc, loss_total / len(data_iter)"],"metadata":{"id":"0p2e1sUzTw-j","executionInfo":{"status":"ok","timestamp":1669474188667,"user_tz":-480,"elapsed":913,"user":{"displayName":"胡博程","userId":"10006746442944693195"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["run函数"],"metadata":{"id":"maOd3-r2UrTD"}},{"cell_type":"code","source":["\n","\n","def fun1(x):\n","    return \" \".join(x)\n","\n","data = pd.read_csv(\"/content/drive/MyDrive/case2/train.news.csv\")\n","data2 = pd.read_csv(\"/content/drive/MyDrive/case2/test.news.csv\")\n","data[\"Report Content\"] = data[\"Report Content\"].apply(lambda x: x.split(\"##\"))\n","data[\"Report Content\"] = data[\"Report Content\"].apply(fun1)\n","data2[\"Report Content\"] = data2[\"Report Content\"].apply(lambda x: x.split(\"##\"))\n","data2[\"Report Content\"] = data2[\"Report Content\"].apply(fun1)\n","\n","train_set, dev_set = train_test_split(data, test_size=0.2, random_state=24)\n","x_train = train_set.iloc[:, [0, 1, 4]]\n","y_train = train_set.iloc[:, 5]\n","x_dev = dev_set.iloc[:, [0, 1, 4]]\n","y_dev = dev_set.iloc[:, 5]\n","x_test = data2.iloc[:, [0, 1, 4]]\n","y_test = data2.iloc[:, 5]\n","\n","t = pd.DataFrame(x_train.astype(str))\n","x_train[\"new\"] = t[\"Title\"] + ' ' + t[\"Ofiicial Account Name\"] + ' ' + t[\"Report Content\"]\n","x_train = x_train.drop([\"Title\", \"Ofiicial Account Name\", \"Report Content\"], axis=1)\n","\n","t = pd.DataFrame(x_dev.astype(str))\n","x_dev[\"new\"] = t[\"Title\"] + ' ' + t[\"Ofiicial Account Name\"] + ' ' + t[\"Report Content\"]\n","x_dev = x_dev.drop([\"Title\", \"Ofiicial Account Name\", \"Report Content\"], axis=1)\n","\n","t = pd.DataFrame(x_test.astype(str))\n","x_test[\"new\"] = t[\"Title\"] + ' ' + t[\"Ofiicial Account Name\"] + ' ' + t[\"Report Content\"]\n","x_test = x_test.drop([\"Title\", \"Ofiicial Account Name\", \"Report Content\"], axis=1)\n","dataset = 'THUCNews'  # 数据集"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v0mcz-T9UscY","executionInfo":{"status":"ok","timestamp":1669474192003,"user_tz":-480,"elapsed":538,"user":{"displayName":"胡博程","userId":"10006746442944693195"}},"outputId":"657a1582-11b7-4fc3-e745-d18777b23365"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]}]},{"cell_type":"markdown","source":["bert模型py里面的两个预备"],"metadata":{"id":"I3-Utaclf93x"}},{"cell_type":"code","source":["# coding=utf-8\n","# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\"\"\"Tokenization classes.\"\"\"\n","\n","from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import collections\n","import logging\n","import os\n","import unicodedata\n","from io import open\n","\n","def cached_path(url_or_filename, cache_dir=None):\n","    \"\"\"\n","    Given something that might be a URL (or might be a local path),\n","    determine which. If it's a URL, download the file and cache it, and\n","    return the path to the cached file. If it's already a local path,\n","    make sure the file exists and then return the path.\n","    \"\"\"\n","    if cache_dir is None:\n","        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n","    if sys.version_info[0] == 3 and isinstance(url_or_filename, Path):\n","        url_or_filename = str(url_or_filename)\n","    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n","        cache_dir = str(cache_dir)\n","\n","    parsed = urlparse(url_or_filename)\n","\n","    if parsed.scheme in ('http', 'https', 's3'):\n","        # URL, so get it from the cache (downloading if necessary)\n","        return get_from_cache(url_or_filename, cache_dir)\n","    elif os.path.exists(url_or_filename):\n","        # File, and it exists.\n","        return url_or_filename\n","    elif parsed.scheme == '':\n","        # File, but it doesn't exist.\n","        raise EnvironmentError(\"file {} not found\".format(url_or_filename))\n","    else:\n","        # Something unknown\n","        raise ValueError(\"unable to parse {} as a URL or as a local path\".format(url_or_filename))\n","\n","\n","logger = logging.getLogger(__name__)\n","\n","PRETRAINED_VOCAB_ARCHIVE_MAP = {\n","    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\",\n","    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt\",\n","    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt\",\n","    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt\",\n","    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt\",\n","    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt\",\n","    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt\",\n","}\n","PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP = {\n","    'bert-base-uncased': 512,\n","    'bert-large-uncased': 512,\n","    'bert-base-cased': 512,\n","    'bert-large-cased': 512,\n","    'bert-base-multilingual-uncased': 512,\n","    'bert-base-multilingual-cased': 512,\n","    'bert-base-chinese': 512,\n","}\n","VOCAB_NAME = 'vocab.txt'\n","\n","\n","def load_vocab(vocab_file):\n","    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n","    vocab = collections.OrderedDict()\n","    index = 0\n","    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n","        while True:\n","            token = reader.readline()\n","            if not token:\n","                break\n","            token = token.strip()\n","            vocab[token] = index\n","            index += 1\n","    return vocab\n","\n","\n","def whitespace_tokenize(text):\n","    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n","    text = text.strip()\n","    if not text:\n","        return []\n","    tokens = text.split()\n","    return tokens\n","\n","\n","class BertTokenizer(object):\n","    \"\"\"Runs end-to-end tokenization: punctuation splitting + wordpiece\"\"\"\n","\n","    def __init__(self, vocab_file, do_lower_case=True, max_len=None, do_basic_tokenize=True,\n","                 never_split=(\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")):\n","        \"\"\"Constructs a BertTokenizer.\n","\n","        Args:\n","          vocab_file: Path to a one-wordpiece-per-line vocabulary file\n","          do_lower_case: Whether to lower case the input\n","                         Only has an effect when do_wordpiece_only=False\n","          do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n","          max_len: An artificial maximum length to truncate tokenized sequences to;\n","                         Effective maximum length is always the minimum of this\n","                         value (if specified) and the underlying BERT model's\n","                         sequence length.\n","          never_split: List of tokens which will never be split during tokenization.\n","                         Only has an effect when do_wordpiece_only=False\n","        \"\"\"\n","        if not os.path.isfile(vocab_file):\n","            raise ValueError(\n","                \"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"\n","                \"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(vocab_file))\n","        self.vocab = load_vocab(vocab_file)\n","        self.ids_to_tokens = collections.OrderedDict(\n","            [(ids, tok) for tok, ids in self.vocab.items()])\n","        self.do_basic_tokenize = do_basic_tokenize\n","        if do_basic_tokenize:\n","          self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n","                                                never_split=never_split)\n","        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n","        self.max_len = max_len if max_len is not None else int(1e12)\n","\n","    def tokenize(self, text):\n","        split_tokens = []\n","        if self.do_basic_tokenize:\n","            for token in self.basic_tokenizer.tokenize(text):\n","                for sub_token in self.wordpiece_tokenizer.tokenize(token):\n","                    split_tokens.append(sub_token)\n","        else:\n","            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n","        return split_tokens\n","\n","    def convert_tokens_to_ids(self, tokens):\n","        \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n","        ids = []\n","        for token in tokens:\n","            ids.append(self.vocab[token])\n","        if len(ids) > self.max_len:\n","            logger.warning(\n","                \"Token indices sequence length is longer than the specified maximum \"\n","                \" sequence length for this BERT model ({} > {}). Running this\"\n","                \" sequence through BERT will result in indexing errors\".format(len(ids), self.max_len)\n","            )\n","        return ids\n","\n","    def convert_ids_to_tokens(self, ids):\n","        \"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"\n","        tokens = []\n","        for i in ids:\n","            tokens.append(self.ids_to_tokens[i])\n","        return tokens\n","\n","    def save_vocabulary(self, vocab_path):\n","        \"\"\"Save the tokenizer vocabulary to a directory or file.\"\"\"\n","        index = 0\n","        if os.path.isdir(vocab_path):\n","            vocab_file = os.path.join(vocab_path, VOCAB_NAME)\n","        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n","            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n","                if index != token_index:\n","                    logger.warning(\"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n","                                   \" Please check that the vocabulary is not corrupted!\".format(vocab_file))\n","                    index = token_index\n","                writer.write(token + u'\\n')\n","                index += 1\n","        return vocab_file\n","\n","    @classmethod\n","    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n","        \"\"\"\n","        Instantiate a PreTrainedBertModel from a pre-trained model file.\n","        Download and cache the pre-trained model file if needed.\n","        \"\"\"\n","        if pretrained_model_name_or_path in PRETRAINED_VOCAB_ARCHIVE_MAP:\n","            vocab_file = PRETRAINED_VOCAB_ARCHIVE_MAP[pretrained_model_name_or_path]\n","            if '-cased' in pretrained_model_name_or_path and kwargs.get('do_lower_case', True):\n","                logger.warning(\"The pre-trained model you are loading is a cased model but you have not set \"\n","                               \"`do_lower_case` to False. We are setting `do_lower_case=False` for you but \"\n","                               \"you may want to check this behavior.\")\n","                kwargs['do_lower_case'] = False\n","            elif '-cased' not in pretrained_model_name_or_path and not kwargs.get('do_lower_case', True):\n","                logger.warning(\"The pre-trained model you are loading is an uncased model but you have set \"\n","                               \"`do_lower_case` to False. We are setting `do_lower_case=True` for you \"\n","                               \"but you may want to check this behavior.\")\n","                kwargs['do_lower_case'] = True\n","        else:\n","            vocab_file = pretrained_model_name_or_path\n","        if os.path.isdir(vocab_file):\n","            vocab_file = os.path.join(vocab_file, VOCAB_NAME)\n","        # redirect to the cache, if necessary\n","        try:\n","            resolved_vocab_file = cached_path(vocab_file, cache_dir=cache_dir)\n","        except EnvironmentError:\n","            logger.error(\n","                \"Model name '{}' was not found in model name list ({}). \"\n","                \"We assumed '{}' was a path or url but couldn't find any file \"\n","                \"associated to this path or url.\".format(\n","                    pretrained_model_name_or_path,\n","                    ', '.join(PRETRAINED_VOCAB_ARCHIVE_MAP.keys()),\n","                    vocab_file))\n","            return None\n","        if resolved_vocab_file == vocab_file:\n","            logger.info(\"loading vocabulary file {}\".format(vocab_file))\n","        else:\n","            logger.info(\"loading vocabulary file {} from cache at {}\".format(\n","                vocab_file, resolved_vocab_file))\n","        if pretrained_model_name_or_path in PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP:\n","            # if we're using a pretrained model, ensure the tokenizer wont index sequences longer\n","            # than the number of positional embeddings\n","            max_len = PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP[pretrained_model_name_or_path]\n","            kwargs['max_len'] = min(kwargs.get('max_len', int(1e12)), max_len)\n","        # Instantiate tokenizer.\n","        tokenizer = cls(resolved_vocab_file, *inputs, **kwargs)\n","        return tokenizer\n","\n","\n","class BasicTokenizer(object):\n","    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n","\n","    def __init__(self,\n","                 do_lower_case=True,\n","                 never_split=(\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")):\n","        \"\"\"Constructs a BasicTokenizer.\n","\n","        Args:\n","          do_lower_case: Whether to lower case the input.\n","        \"\"\"\n","        self.do_lower_case = do_lower_case\n","        self.never_split = never_split\n","\n","    def tokenize(self, text):\n","        \"\"\"Tokenizes a piece of text.\"\"\"\n","        text = self._clean_text(text)\n","        # This was added on November 1st, 2018 for the multilingual and Chinese\n","        # models. This is also applied to the English models now, but it doesn't\n","        # matter since the English models were not trained on any Chinese data\n","        # and generally don't have any Chinese data in them (there are Chinese\n","        # characters in the vocabulary because Wikipedia does have some Chinese\n","        # words in the English Wikipedia.).\n","        text = self._tokenize_chinese_chars(text)\n","        orig_tokens = whitespace_tokenize(text)\n","        split_tokens = []\n","        for token in orig_tokens:\n","            if self.do_lower_case and token not in self.never_split:\n","                token = token.lower()\n","                token = self._run_strip_accents(token)\n","            split_tokens.extend(self._run_split_on_punc(token))\n","\n","        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n","        return output_tokens\n","\n","    def _run_strip_accents(self, text):\n","        \"\"\"Strips accents from a piece of text.\"\"\"\n","        text = unicodedata.normalize(\"NFD\", text)\n","        output = []\n","        for char in text:\n","            cat = unicodedata.category(char)\n","            if cat == \"Mn\":\n","                continue\n","            output.append(char)\n","        return \"\".join(output)\n","\n","    def _run_split_on_punc(self, text):\n","        \"\"\"Splits punctuation on a piece of text.\"\"\"\n","        if text in self.never_split:\n","            return [text]\n","        chars = list(text)\n","        i = 0\n","        start_new_word = True\n","        output = []\n","        while i < len(chars):\n","            char = chars[i]\n","            if _is_punctuation(char):\n","                output.append([char])\n","                start_new_word = True\n","            else:\n","                if start_new_word:\n","                    output.append([])\n","                start_new_word = False\n","                output[-1].append(char)\n","            i += 1\n","\n","        return [\"\".join(x) for x in output]\n","\n","    def _tokenize_chinese_chars(self, text):\n","        \"\"\"Adds whitespace around any CJK character.\"\"\"\n","        output = []\n","        for char in text:\n","            cp = ord(char)\n","            if self._is_chinese_char(cp):\n","                output.append(\" \")\n","                output.append(char)\n","                output.append(\" \")\n","            else:\n","                output.append(char)\n","        return \"\".join(output)\n","\n","    def _is_chinese_char(self, cp):\n","        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n","        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n","        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n","        #\n","        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n","        # despite its name. The modern Korean Hangul alphabet is a different block,\n","        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n","        # space-separated words, so they are not treated specially and handled\n","        # like the all of the other languages.\n","        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n","                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n","                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n","                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n","                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n","                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n","                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n","                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n","            return True\n","\n","        return False\n","\n","    def _clean_text(self, text):\n","        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n","        output = []\n","        for char in text:\n","            cp = ord(char)\n","            if cp == 0 or cp == 0xfffd or _is_control(char):\n","                continue\n","            if _is_whitespace(char):\n","                output.append(\" \")\n","            else:\n","                output.append(char)\n","        return \"\".join(output)\n","\n","\n","class WordpieceTokenizer(object):\n","    \"\"\"Runs WordPiece tokenization.\"\"\"\n","\n","    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=100):\n","        self.vocab = vocab\n","        self.unk_token = unk_token\n","        self.max_input_chars_per_word = max_input_chars_per_word\n","\n","    def tokenize(self, text):\n","        \"\"\"Tokenizes a piece of text into its word pieces.\n","\n","        This uses a greedy longest-match-first algorithm to perform tokenization\n","        using the given vocabulary.\n","\n","        For example:\n","          input = \"unaffable\"\n","          output = [\"un\", \"##aff\", \"##able\"]\n","\n","        Args:\n","          text: A single token or whitespace separated tokens. This should have\n","            already been passed through `BasicTokenizer`.\n","\n","        Returns:\n","          A list of wordpiece tokens.\n","        \"\"\"\n","\n","        output_tokens = []\n","        for token in whitespace_tokenize(text):\n","            chars = list(token)\n","            if len(chars) > self.max_input_chars_per_word:\n","                output_tokens.append(self.unk_token)\n","                continue\n","\n","            is_bad = False\n","            start = 0\n","            sub_tokens = []\n","            while start < len(chars):\n","                end = len(chars)\n","                cur_substr = None\n","                while start < end:\n","                    substr = \"\".join(chars[start:end])\n","                    if start > 0:\n","                        substr = \"##\" + substr\n","                    if substr in self.vocab:\n","                        cur_substr = substr\n","                        break\n","                    end -= 1\n","                if cur_substr is None:\n","                    is_bad = True\n","                    break\n","                sub_tokens.append(cur_substr)\n","                start = end\n","\n","            if is_bad:\n","                output_tokens.append(self.unk_token)\n","            else:\n","                output_tokens.extend(sub_tokens)\n","        return output_tokens\n","\n","\n","def _is_whitespace(char):\n","    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n","    # \\t, \\n, and \\r are technically contorl characters but we treat them\n","    # as whitespace since they are generally considered as such.\n","    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n","        return True\n","    cat = unicodedata.category(char)\n","    if cat == \"Zs\":\n","        return True\n","    return False\n","\n","\n","def _is_control(char):\n","    \"\"\"Checks whether `chars` is a control character.\"\"\"\n","    # These are technically control characters but we count them as whitespace\n","    # characters.\n","    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n","        return False\n","    cat = unicodedata.category(char)\n","    if cat.startswith(\"C\"):\n","        return True\n","    return False\n","\n","\n","def _is_punctuation(char):\n","    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n","    cp = ord(char)\n","    # We treat all non-letter/number ASCII as punctuation.\n","    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n","    # Punctuation class but we treat them as punctuation anyways, for\n","    # consistency.\n","    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n","            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n","        return True\n","    cat = unicodedata.category(char)\n","    if cat.startswith(\"P\"):\n","        return True\n","    return False\n"],"metadata":{"id":"8HT8MC0Vf9Iu","executionInfo":{"status":"ok","timestamp":1669474194334,"user_tz":-480,"elapsed":2,"user":{"displayName":"胡博程","userId":"10006746442944693195"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# coding=utf-8\n","# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n","# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\"\"\"PyTorch BERT model.\"\"\"\n","\n","from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import copy\n","import json\n","import logging\n","import math\n","import os\n","import shutil\n","import tarfile\n","import tempfile\n","import sys\n","from io import open\n","\n","import torch\n","from torch import nn\n","from torch.nn import CrossEntropyLoss\n","\n","CONFIG_NAME = \"config.json\"\n","WEIGHTS_NAME = \"pytorch_model.bin\"\n","\n","logger = logging.getLogger(__name__)\n","\n","PRETRAINED_MODEL_ARCHIVE_MAP = {\n","    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz\",\n","    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz\",\n","    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz\",\n","    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz\",\n","    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz\",\n","    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz\",\n","    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz\",\n","}\n","BERT_CONFIG_NAME = 'bert_config.json'\n","TF_WEIGHTS_NAME = 'model.ckpt'\n","\n","def load_tf_weights_in_bert(model, tf_checkpoint_path):\n","    \"\"\" Load tf checkpoints in a pytorch model\n","    \"\"\"\n","    try:\n","        import re\n","        import numpy as np\n","        import tensorflow as tf\n","    except ImportError:\n","        print(\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"\n","            \"https://www.tensorflow.org/install/ for installation instructions.\")\n","        raise\n","    tf_path = os.path.abspath(tf_checkpoint_path)\n","    print(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n","    # Load weights from TF model\n","    init_vars = tf.train.list_variables(tf_path)\n","    names = []\n","    arrays = []\n","    for name, shape in init_vars:\n","        print(\"Loading TF weight {} with shape {}\".format(name, shape))\n","        array = tf.train.load_variable(tf_path, name)\n","        names.append(name)\n","        arrays.append(array)\n","\n","    for name, array in zip(names, arrays):\n","        name = name.split('/')\n","        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n","        # which are not required for using pretrained model\n","        if any(n in [\"adam_v\", \"adam_m\", \"global_step\"] for n in name):\n","            print(\"Skipping {}\".format(\"/\".join(name)))\n","            continue\n","        pointer = model\n","        for m_name in name:\n","            if re.fullmatch(r'[A-Za-z]+_\\d+', m_name):\n","                l = re.split(r'_(\\d+)', m_name)\n","            else:\n","                l = [m_name]\n","            if l[0] == 'kernel' or l[0] == 'gamma':\n","                pointer = getattr(pointer, 'weight')\n","            elif l[0] == 'output_bias' or l[0] == 'beta':\n","                pointer = getattr(pointer, 'bias')\n","            elif l[0] == 'output_weights':\n","                pointer = getattr(pointer, 'weight')\n","            elif l[0] == 'squad':\n","                pointer = getattr(pointer, 'classifier')\n","            else:\n","                try:\n","                    pointer = getattr(pointer, l[0])\n","                except AttributeError:\n","                    print(\"Skipping {}\".format(\"/\".join(name)))\n","                    continue\n","            if len(l) >= 2:\n","                num = int(l[1])\n","                pointer = pointer[num]\n","        if m_name[-11:] == '_embeddings':\n","            pointer = getattr(pointer, 'weight')\n","        elif m_name == 'kernel':\n","            array = np.transpose(array)\n","        try:\n","            assert pointer.shape == array.shape\n","        except AssertionError as e:\n","            e.args += (pointer.shape, array.shape)\n","            raise\n","        print(\"Initialize PyTorch weight {}\".format(name))\n","        pointer.data = torch.from_numpy(array)\n","    return model\n","\n","\n","def gelu(x):\n","    \"\"\"Implementation of the gelu activation function.\n","        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n","        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n","        Also see https://arxiv.org/abs/1606.08415\n","    \"\"\"\n","    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n","\n","\n","def swish(x):\n","    return x * torch.sigmoid(x)\n","\n","\n","ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n","\n","\n","class BertConfig(object):\n","    \"\"\"Configuration class to store the configuration of a `BertModel`.\n","    \"\"\"\n","    def __init__(self,\n","                 vocab_size_or_config_json_file,\n","                 hidden_size=768,\n","                 num_hidden_layers=12,\n","                 num_attention_heads=12,\n","                 intermediate_size=3072,\n","                 hidden_act=\"gelu\",\n","                 hidden_dropout_prob=0.1,\n","                 attention_probs_dropout_prob=0.1,\n","                 max_position_embeddings=512,\n","                 type_vocab_size=2,\n","                 initializer_range=0.02):\n","        \"\"\"Constructs BertConfig.\n","\n","        Args:\n","            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n","            hidden_size: Size of the encoder layers and the pooler layer.\n","            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n","            num_attention_heads: Number of attention heads for each attention layer in\n","                the Transformer encoder.\n","            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n","                layer in the Transformer encoder.\n","            hidden_act: The non-linear activation function (function or string) in the\n","                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n","            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n","                layers in the embeddings, encoder, and pooler.\n","            attention_probs_dropout_prob: The dropout ratio for the attention\n","                probabilities.\n","            max_position_embeddings: The maximum sequence length that this model might\n","                ever be used with. Typically set this to something large just in case\n","                (e.g., 512 or 1024 or 2048).\n","            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n","                `BertModel`.\n","            initializer_range: The sttdev of the truncated_normal_initializer for\n","                initializing all weight matrices.\n","        \"\"\"\n","        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n","                        and isinstance(vocab_size_or_config_json_file, unicode)):\n","            with open(vocab_size_or_config_json_file, \"r\", encoding='utf-8') as reader:\n","                json_config = json.loads(reader.read())\n","            for key, value in json_config.items():\n","                self.__dict__[key] = value\n","        elif isinstance(vocab_size_or_config_json_file, int):\n","            self.vocab_size = vocab_size_or_config_json_file\n","            self.hidden_size = hidden_size\n","            self.num_hidden_layers = num_hidden_layers\n","            self.num_attention_heads = num_attention_heads\n","            self.hidden_act = hidden_act\n","            self.intermediate_size = intermediate_size\n","            self.hidden_dropout_prob = hidden_dropout_prob\n","            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n","            self.max_position_embeddings = max_position_embeddings\n","            self.type_vocab_size = type_vocab_size\n","            self.initializer_range = initializer_range\n","        else:\n","            raise ValueError(\"First argument must be either a vocabulary size (int)\"\n","                             \"or the path to a pretrained model config file (str)\")\n","\n","    @classmethod\n","    def from_dict(cls, json_object):\n","        \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n","        config = BertConfig(vocab_size_or_config_json_file=-1)\n","        for key, value in json_object.items():\n","            config.__dict__[key] = value\n","        return config\n","\n","    @classmethod\n","    def from_json_file(cls, json_file):\n","        \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n","        with open(json_file, \"r\", encoding='utf-8') as reader:\n","            text = reader.read()\n","        return cls.from_dict(json.loads(text))\n","\n","    def __repr__(self):\n","        return str(self.to_json_string())\n","\n","    def to_dict(self):\n","        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n","        output = copy.deepcopy(self.__dict__)\n","        return output\n","\n","    def to_json_string(self):\n","        \"\"\"Serializes this instance to a JSON string.\"\"\"\n","        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n","\n","    def to_json_file(self, json_file_path):\n","        \"\"\" Save this instance to a json file.\"\"\"\n","        with open(json_file_path, \"w\", encoding='utf-8') as writer:\n","            writer.write(self.to_json_string())\n","\n","try:\n","    from apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm\n","except ImportError:\n","    logger.info(\"Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\")\n","    class BertLayerNorm(nn.Module):\n","        def __init__(self, hidden_size, eps=1e-12):\n","            \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n","            \"\"\"\n","            super(BertLayerNorm, self).__init__()\n","            self.weight = nn.Parameter(torch.ones(hidden_size))\n","            self.bias = nn.Parameter(torch.zeros(hidden_size))\n","            self.variance_epsilon = eps\n","\n","        def forward(self, x):\n","            u = x.mean(-1, keepdim=True)\n","            s = (x - u).pow(2).mean(-1, keepdim=True)\n","            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n","            return self.weight * x + self.bias\n","\n","class BertEmbeddings(nn.Module):\n","    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n","    \"\"\"\n","    def __init__(self, config):\n","        super(BertEmbeddings, self).__init__()\n","        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n","        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n","        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n","\n","        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n","        # any TensorFlow checkpoint file\n","        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(self, input_ids, token_type_ids=None):\n","        seq_length = input_ids.size(1)\n","        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n","        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","\n","        words_embeddings = self.word_embeddings(input_ids)\n","        position_embeddings = self.position_embeddings(position_ids)\n","        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n","\n","        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n","        embeddings = self.LayerNorm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","        return embeddings\n","\n","\n","class BertSelfAttention(nn.Module):\n","    def __init__(self, config):\n","        super(BertSelfAttention, self).__init__()\n","        if config.hidden_size % config.num_attention_heads != 0:\n","            raise ValueError(\n","                \"The hidden size (%d) is not a multiple of the number of attention \"\n","                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n","        self.num_attention_heads = config.num_attention_heads\n","        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n","        self.all_head_size = self.num_attention_heads * self.attention_head_size\n","\n","        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n","        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n","        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n","\n","        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n","\n","    def transpose_for_scores(self, x):\n","        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n","        x = x.view(*new_x_shape)\n","        return x.permute(0, 2, 1, 3)\n","\n","    def forward(self, hidden_states, attention_mask):\n","        mixed_query_layer = self.query(hidden_states)\n","        mixed_key_layer = self.key(hidden_states)\n","        mixed_value_layer = self.value(hidden_states)\n","\n","        query_layer = self.transpose_for_scores(mixed_query_layer)\n","        key_layer = self.transpose_for_scores(mixed_key_layer)\n","        value_layer = self.transpose_for_scores(mixed_value_layer)\n","\n","        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n","        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n","        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n","        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n","        attention_scores = attention_scores + attention_mask\n","\n","        # Normalize the attention scores to probabilities.\n","        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n","\n","        # This is actually dropping out entire tokens to attend to, which might\n","        # seem a bit unusual, but is taken from the original Transformer paper.\n","        attention_probs = self.dropout(attention_probs)\n","\n","        context_layer = torch.matmul(attention_probs, value_layer)\n","        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n","        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n","        context_layer = context_layer.view(*new_context_layer_shape)\n","        return context_layer\n","\n","\n","class BertSelfOutput(nn.Module):\n","    def __init__(self, config):\n","        super(BertSelfOutput, self).__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(self, hidden_states, input_tensor):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.dropout(hidden_states)\n","        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n","        return hidden_states\n","\n","\n","class BertAttention(nn.Module):\n","    def __init__(self, config):\n","        super(BertAttention, self).__init__()\n","        self.self = BertSelfAttention(config)\n","        self.output = BertSelfOutput(config)\n","\n","    def forward(self, input_tensor, attention_mask):\n","        self_output = self.self(input_tensor, attention_mask)\n","        attention_output = self.output(self_output, input_tensor)\n","        return attention_output\n","\n","\n","class BertIntermediate(nn.Module):\n","    def __init__(self, config):\n","        super(BertIntermediate, self).__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n","        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n","            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n","        else:\n","            self.intermediate_act_fn = config.hidden_act\n","\n","    def forward(self, hidden_states):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.intermediate_act_fn(hidden_states)\n","        return hidden_states\n","\n","\n","class BertOutput(nn.Module):\n","    def __init__(self, config):\n","        super(BertOutput, self).__init__()\n","        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n","        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(self, hidden_states, input_tensor):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.dropout(hidden_states)\n","        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n","        return hidden_states\n","\n","\n","class BertLayer(nn.Module):\n","    def __init__(self, config):\n","        super(BertLayer, self).__init__()\n","        self.attention = BertAttention(config)\n","        self.intermediate = BertIntermediate(config)\n","        self.output = BertOutput(config)\n","\n","    def forward(self, hidden_states, attention_mask):\n","        attention_output = self.attention(hidden_states, attention_mask)\n","        intermediate_output = self.intermediate(attention_output)\n","        layer_output = self.output(intermediate_output, attention_output)\n","        return layer_output\n","\n","\n","class BertEncoder(nn.Module):\n","    def __init__(self, config):\n","        super(BertEncoder, self).__init__()\n","        layer = BertLayer(config)\n","        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n","\n","    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n","        all_encoder_layers = []\n","        for layer_module in self.layer:\n","            hidden_states = layer_module(hidden_states, attention_mask)\n","            if output_all_encoded_layers:\n","                all_encoder_layers.append(hidden_states)\n","        if not output_all_encoded_layers:\n","            all_encoder_layers.append(hidden_states)\n","        return all_encoder_layers\n","\n","\n","class BertPooler(nn.Module):\n","    def __init__(self, config):\n","        super(BertPooler, self).__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.activation = nn.Tanh()\n","\n","    def forward(self, hidden_states):\n","        # We \"pool\" the model by simply taking the hidden state corresponding\n","        # to the first token.\n","        first_token_tensor = hidden_states[:, 0]\n","        pooled_output = self.dense(first_token_tensor)\n","        pooled_output = self.activation(pooled_output)\n","        return pooled_output\n","\n","\n","class BertPredictionHeadTransform(nn.Module):\n","    def __init__(self, config):\n","        super(BertPredictionHeadTransform, self).__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n","            self.transform_act_fn = ACT2FN[config.hidden_act]\n","        else:\n","            self.transform_act_fn = config.hidden_act\n","        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n","\n","    def forward(self, hidden_states):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.transform_act_fn(hidden_states)\n","        hidden_states = self.LayerNorm(hidden_states)\n","        return hidden_states\n","\n","\n","class BertLMPredictionHead(nn.Module):\n","    def __init__(self, config, bert_model_embedding_weights):\n","        super(BertLMPredictionHead, self).__init__()\n","        self.transform = BertPredictionHeadTransform(config)\n","\n","        # The output weights are the same as the input embeddings, but there is\n","        # an output-only bias for each token.\n","        self.decoder = nn.Linear(bert_model_embedding_weights.size(1),\n","                                 bert_model_embedding_weights.size(0),\n","                                 bias=False)\n","        self.decoder.weight = bert_model_embedding_weights\n","        self.bias = nn.Parameter(torch.zeros(bert_model_embedding_weights.size(0)))\n","\n","    def forward(self, hidden_states):\n","        hidden_states = self.transform(hidden_states)\n","        hidden_states = self.decoder(hidden_states) + self.bias\n","        return hidden_states\n","\n","\n","class BertOnlyMLMHead(nn.Module):\n","    def __init__(self, config, bert_model_embedding_weights):\n","        super(BertOnlyMLMHead, self).__init__()\n","        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n","\n","    def forward(self, sequence_output):\n","        prediction_scores = self.predictions(sequence_output)\n","        return prediction_scores\n","\n","\n","class BertOnlyNSPHead(nn.Module):\n","    def __init__(self, config):\n","        super(BertOnlyNSPHead, self).__init__()\n","        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n","\n","    def forward(self, pooled_output):\n","        seq_relationship_score = self.seq_relationship(pooled_output)\n","        return seq_relationship_score\n","\n","\n","class BertPreTrainingHeads(nn.Module):\n","    def __init__(self, config, bert_model_embedding_weights):\n","        super(BertPreTrainingHeads, self).__init__()\n","        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n","        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n","\n","    def forward(self, sequence_output, pooled_output):\n","        prediction_scores = self.predictions(sequence_output)\n","        seq_relationship_score = self.seq_relationship(pooled_output)\n","        return prediction_scores, seq_relationship_score\n","\n","\n","class BertPreTrainedModel(nn.Module):\n","    \"\"\" An abstract class to handle weights initialization and\n","        a simple interface for dowloading and loading pretrained models.\n","    \"\"\"\n","    def __init__(self, config, *inputs, **kwargs):\n","        super(BertPreTrainedModel, self).__init__()\n","        if not isinstance(config, BertConfig):\n","            raise ValueError(\n","                \"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"\n","                \"To create a model from a Google pretrained model use \"\n","                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n","                    self.__class__.__name__, self.__class__.__name__\n","                ))\n","        self.config = config\n","\n","    def init_bert_weights(self, module):\n","        \"\"\" Initialize the weights.\n","        \"\"\"\n","        if isinstance(module, (nn.Linear, nn.Embedding)):\n","            # Slightly different from the TF version which uses truncated_normal for initialization\n","            # cf https://github.com/pytorch/pytorch/pull/5617\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","        elif isinstance(module, BertLayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","        if isinstance(module, nn.Linear) and module.bias is not None:\n","            module.bias.data.zero_()\n","\n","    @classmethod\n","    def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n","        \"\"\"\n","        Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\n","        Download and cache the pre-trained model file if needed.\n","\n","        Params:\n","            pretrained_model_name_or_path: either:\n","                - a str with the name of a pre-trained model to load selected in the list of:\n","                    . `bert-base-uncased`\n","                    . `bert-large-uncased`\n","                    . `bert-base-cased`\n","                    . `bert-large-cased`\n","                    . `bert-base-multilingual-uncased`\n","                    . `bert-base-multilingual-cased`\n","                    . `bert-base-chinese`\n","                - a path or url to a pretrained model archive containing:\n","                    . `bert_config.json` a configuration file for the model\n","                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n","                - a path or url to a pretrained model archive containing:\n","                    . `bert_config.json` a configuration file for the model\n","                    . `model.chkpt` a TensorFlow checkpoint\n","            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n","            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n","            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n","            *inputs, **kwargs: additional input for the specific Bert class\n","                (ex: num_labels for BertForSequenceClassification)\n","        \"\"\"\n","        state_dict = kwargs.get('state_dict', None)\n","        kwargs.pop('state_dict', None)\n","        cache_dir = kwargs.get('cache_dir', None)\n","        kwargs.pop('cache_dir', None)\n","        from_tf = kwargs.get('from_tf', False)\n","        kwargs.pop('from_tf', None)\n","\n","        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n","            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n","        else:\n","            archive_file = pretrained_model_name_or_path\n","        # redirect to the cache, if necessary\n","        try:\n","            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n","        except EnvironmentError:\n","            logger.error(\n","                \"Model name '{}' was not found in model name list ({}). \"\n","                \"We assumed '{}' was a path or url but couldn't find any file \"\n","                \"associated to this path or url.\".format(\n","                    pretrained_model_name_or_path,\n","                    ', '.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n","                    archive_file))\n","            return None\n","        if resolved_archive_file == archive_file:\n","            logger.info(\"loading archive file {}\".format(archive_file))\n","        else:\n","            logger.info(\"loading archive file {} from cache at {}\".format(\n","                archive_file, resolved_archive_file))\n","        tempdir = None\n","        if os.path.isdir(resolved_archive_file) or from_tf:\n","            serialization_dir = resolved_archive_file\n","        else:\n","            # Extract archive to temp dir\n","            tempdir = tempfile.mkdtemp()\n","            logger.info(\"extracting archive file {} to temp dir {}\".format(\n","                resolved_archive_file, tempdir))\n","            with tarfile.open(resolved_archive_file, 'r:gz') as archive:\n","                archive.extractall(tempdir)\n","            serialization_dir = tempdir\n","        # Load config\n","        config_file = os.path.join(serialization_dir, CONFIG_NAME)\n","        if not os.path.exists(config_file):\n","            # Backward compatibility with old naming format\n","            config_file = os.path.join(serialization_dir, BERT_CONFIG_NAME)\n","        config = BertConfig.from_json_file(config_file)\n","        logger.info(\"Model config {}\".format(config))\n","        # Instantiate model.\n","        model = cls(config, *inputs, **kwargs)\n","        if state_dict is None and not from_tf:\n","            weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\n","            state_dict = torch.load(weights_path, map_location='cpu')\n","        if tempdir:\n","            # Clean up temp dir\n","            shutil.rmtree(tempdir)\n","        if from_tf:\n","            # Directly load from a TensorFlow checkpoint\n","            weights_path = os.path.join(serialization_dir, TF_WEIGHTS_NAME)\n","            return load_tf_weights_in_bert(model, weights_path)\n","        # Load from a PyTorch state_dict\n","        old_keys = []\n","        new_keys = []\n","        for key in state_dict.keys():\n","            new_key = None\n","            if 'gamma' in key:\n","                new_key = key.replace('gamma', 'weight')\n","            if 'beta' in key:\n","                new_key = key.replace('beta', 'bias')\n","            if new_key:\n","                old_keys.append(key)\n","                new_keys.append(new_key)\n","        for old_key, new_key in zip(old_keys, new_keys):\n","            state_dict[new_key] = state_dict.pop(old_key)\n","\n","        missing_keys = []\n","        unexpected_keys = []\n","        error_msgs = []\n","        # copy state_dict so _load_from_state_dict can modify it\n","        metadata = getattr(state_dict, '_metadata', None)\n","        state_dict = state_dict.copy()\n","        if metadata is not None:\n","            state_dict._metadata = metadata\n","\n","        def load(module, prefix=''):\n","            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n","            module._load_from_state_dict(\n","                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n","            for name, child in module._modules.items():\n","                if child is not None:\n","                    load(child, prefix + name + '.')\n","        start_prefix = ''\n","        if not hasattr(model, 'bert') and any(s.startswith('bert.') for s in state_dict.keys()):\n","            start_prefix = 'bert.'\n","        load(model, prefix=start_prefix)\n","        if len(missing_keys) > 0:\n","            logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(\n","                model.__class__.__name__, missing_keys))\n","        if len(unexpected_keys) > 0:\n","            logger.info(\"Weights from pretrained model not used in {}: {}\".format(\n","                model.__class__.__name__, unexpected_keys))\n","        if len(error_msgs) > 0:\n","            raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n","                               model.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n","        return model\n","\n","\n","class BertModel(BertPreTrainedModel):\n","    \"\"\"BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n","\n","    Params:\n","        config: a BertConfig class instance with the configuration to build a new model\n","\n","    Inputs:\n","        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n","            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n","            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n","        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n","            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n","            a `sentence B` token (see BERT paper for more details).\n","        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n","            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n","            input sequence length in the current batch. It's the mask that we typically use for attention when\n","            a batch has varying length sentences.\n","        `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n","\n","    Outputs: Tuple of (encoded_layers, pooled_output)\n","        `encoded_layers`: controled by `output_all_encoded_layers` argument:\n","            - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n","                of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n","                encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n","            - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n","                to the last attention block of shape [batch_size, sequence_length, hidden_size],\n","        `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n","            classifier pretrained on top of the hidden state associated to the first character of the\n","            input (`CLS`) to train on the Next-Sentence task (see BERT's paper).\n","\n","    Example usage:\n","    ```python\n","    # Already been converted into WordPiece token ids\n","    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n","    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n","    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n","\n","    config = modeling.BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n","        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n","\n","    model = modeling.BertModel(config=config)\n","    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n","    ```\n","    \"\"\"\n","    def __init__(self, config):\n","        super(BertModel, self).__init__(config)\n","        self.embeddings = BertEmbeddings(config)\n","        self.encoder = BertEncoder(config)\n","        self.pooler = BertPooler(config)\n","        self.apply(self.init_bert_weights)\n","\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True):\n","        if attention_mask is None:\n","            attention_mask = torch.ones_like(input_ids)\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","\n","        # We create a 3D attention mask from a 2D tensor mask.\n","        # Sizes are [batch_size, 1, 1, to_seq_length]\n","        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n","        # this attention mask is more simple than the triangular masking of causal attention\n","        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n","        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n","\n","        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n","        # masked positions, this operation will create a tensor which is 0.0 for\n","        # positions we want to attend and -10000.0 for masked positions.\n","        # Since we are adding it to the raw scores before the softmax, this is\n","        # effectively the same as removing these entirely.\n","        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","\n","        embedding_output = self.embeddings(input_ids, token_type_ids)\n","        encoded_layers = self.encoder(embedding_output,\n","                                      extended_attention_mask,\n","                                      output_all_encoded_layers=output_all_encoded_layers)\n","        sequence_output = encoded_layers[-1]\n","        pooled_output = self.pooler(sequence_output)\n","        if not output_all_encoded_layers:\n","            encoded_layers = encoded_layers[-1]\n","        return encoded_layers, pooled_output\n","\n","\n","class BertForPreTraining(BertPreTrainedModel):\n","    \"\"\"BERT model with pre-training heads.\n","    This module comprises the BERT model followed by the two pre-training heads:\n","        - the masked language modeling head, and\n","        - the next sentence classification head.\n","\n","    Params:\n","        config: a BertConfig class instance with the configuration to build a new model.\n","\n","    Inputs:\n","        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n","            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n","            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n","        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n","            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n","            a `sentence B` token (see BERT paper for more details).\n","        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n","            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n","            input sequence length in the current batch. It's the mask that we typically use for attention when\n","            a batch has varying length sentences.\n","        `masked_lm_labels`: optional masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n","            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n","            is only computed for the labels set in [0, ..., vocab_size]\n","        `next_sentence_label`: optional next sentence classification loss: torch.LongTensor of shape [batch_size]\n","            with indices selected in [0, 1].\n","            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n","\n","    Outputs:\n","        if `masked_lm_labels` and `next_sentence_label` are not `None`:\n","            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n","            sentence classification loss.\n","        if `masked_lm_labels` or `next_sentence_label` is `None`:\n","            Outputs a tuple comprising\n","            - the masked language modeling logits of shape [batch_size, sequence_length, vocab_size], and\n","            - the next sentence classification logits of shape [batch_size, 2].\n","\n","    Example usage:\n","    ```python\n","    # Already been converted into WordPiece token ids\n","    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n","    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n","    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n","\n","    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n","        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n","\n","    model = BertForPreTraining(config)\n","    masked_lm_logits_scores, seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n","    ```\n","    \"\"\"\n","    def __init__(self, config):\n","        super(BertForPreTraining, self).__init__(config)\n","        self.bert = BertModel(config)\n","        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n","        self.apply(self.init_bert_weights)\n","\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None):\n","        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n","                                                   output_all_encoded_layers=False)\n","        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n","\n","        if masked_lm_labels is not None and next_sentence_label is not None:\n","            loss_fct = CrossEntropyLoss(ignore_index=-1)\n","            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n","            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n","            total_loss = masked_lm_loss + next_sentence_loss\n","            return total_loss\n","        else:\n","            return prediction_scores, seq_relationship_score\n","\n","\n","class BertForMaskedLM(BertPreTrainedModel):\n","    \"\"\"BERT model with the masked language modeling head.\n","    This module comprises the BERT model followed by the masked language modeling head.\n","\n","    Params:\n","        config: a BertConfig class instance with the configuration to build a new model.\n","\n","    Inputs:\n","        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n","            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n","            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n","        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n","            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n","            a `sentence B` token (see BERT paper for more details).\n","        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n","            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n","            input sequence length in the current batch. It's the mask that we typically use for attention when\n","            a batch has varying length sentences.\n","        `masked_lm_labels`: masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n","            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n","            is only computed for the labels set in [0, ..., vocab_size]\n","\n","    Outputs:\n","        if `masked_lm_labels` is  not `None`:\n","            Outputs the masked language modeling loss.\n","        if `masked_lm_labels` is `None`:\n","            Outputs the masked language modeling logits of shape [batch_size, sequence_length, vocab_size].\n","\n","    Example usage:\n","    ```python\n","    # Already been converted into WordPiece token ids\n","    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n","    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n","    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n","\n","    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n","        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n","\n","    model = BertForMaskedLM(config)\n","    masked_lm_logits_scores = model(input_ids, token_type_ids, input_mask)\n","    ```\n","    \"\"\"\n","    def __init__(self, config):\n","        super(BertForMaskedLM, self).__init__(config)\n","        self.bert = BertModel(config)\n","        self.cls = BertOnlyMLMHead(config, self.bert.embeddings.word_embeddings.weight)\n","        self.apply(self.init_bert_weights)\n","\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None):\n","        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask,\n","                                       output_all_encoded_layers=False)\n","        prediction_scores = self.cls(sequence_output)\n","\n","        if masked_lm_labels is not None:\n","            loss_fct = CrossEntropyLoss(ignore_index=-1)\n","            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n","            return masked_lm_loss\n","        else:\n","            return prediction_scores\n","\n","\n","class BertForNextSentencePrediction(BertPreTrainedModel):\n","    \"\"\"BERT model with next sentence prediction head.\n","    This module comprises the BERT model followed by the next sentence classification head.\n","\n","    Params:\n","        config: a BertConfig class instance with the configuration to build a new model.\n","\n","    Inputs:\n","        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n","            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n","            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n","        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n","            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n","            a `sentence B` token (see BERT paper for more details).\n","        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n","            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n","            input sequence length in the current batch. It's the mask that we typically use for attention when\n","            a batch has varying length sentences.\n","        `next_sentence_label`: next sentence classification loss: torch.LongTensor of shape [batch_size]\n","            with indices selected in [0, 1].\n","            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n","\n","    Outputs:\n","        if `next_sentence_label` is not `None`:\n","            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n","            sentence classification loss.\n","        if `next_sentence_label` is `None`:\n","            Outputs the next sentence classification logits of shape [batch_size, 2].\n","\n","    Example usage:\n","    ```python\n","    # Already been converted into WordPiece token ids\n","    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n","    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n","    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n","\n","    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n","        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n","\n","    model = BertForNextSentencePrediction(config)\n","    seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n","    ```\n","    \"\"\"\n","    def __init__(self, config):\n","        super(BertForNextSentencePrediction, self).__init__(config)\n","        self.bert = BertModel(config)\n","        self.cls = BertOnlyNSPHead(config)\n","        self.apply(self.init_bert_weights)\n","\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None, next_sentence_label=None):\n","        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n","                                     output_all_encoded_layers=False)\n","        seq_relationship_score = self.cls( pooled_output)\n","\n","        if next_sentence_label is not None:\n","            loss_fct = CrossEntropyLoss(ignore_index=-1)\n","            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n","            return next_sentence_loss\n","        else:\n","            return seq_relationship_score\n","\n","\n","class BertForSequenceClassification(BertPreTrainedModel):\n","    \"\"\"BERT model for classification.\n","    This module is composed of the BERT model with a linear layer on top of\n","    the pooled output.\n","\n","    Params:\n","        `config`: a BertConfig class instance with the configuration to build a new model.\n","        `num_labels`: the number of classes for the classifier. Default = 2.\n","\n","    Inputs:\n","        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n","            with the word token indices in the vocabulary. Items in the batch should begin with the special \"CLS\" token. (see the tokens preprocessing logic in the scripts\n","            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n","        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n","            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n","            a `sentence B` token (see BERT paper for more details).\n","        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n","            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n","            input sequence length in the current batch. It's the mask that we typically use for attention when\n","            a batch has varying length sentences.\n","        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n","            with indices selected in [0, ..., num_labels].\n","\n","    Outputs:\n","        if `labels` is not `None`:\n","            Outputs the CrossEntropy classification loss of the output with the labels.\n","        if `labels` is `None`:\n","            Outputs the classification logits of shape [batch_size, num_labels].\n","\n","    Example usage:\n","    ```python\n","    # Already been converted into WordPiece token ids\n","    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n","    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n","    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n","\n","    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n","        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n","\n","    num_labels = 2\n","\n","    model = BertForSequenceClassification(config, num_labels)\n","    logits = model(input_ids, token_type_ids, input_mask)\n","    ```\n","    \"\"\"\n","    def __init__(self, config, num_labels):\n","        super(BertForSequenceClassification, self).__init__(config)\n","        self.num_labels = num_labels\n","        self.bert = BertModel(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, num_labels)\n","        self.apply(self.init_bert_weights)\n","\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n","        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss()\n","            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            return loss\n","        else:\n","            return logits\n","\n","\n","class BertForMultipleChoice(BertPreTrainedModel):\n","    \"\"\"BERT model for multiple choice tasks.\n","    This module is composed of the BERT model with a linear layer on top of\n","    the pooled output.\n","\n","    Params:\n","        `config`: a BertConfig class instance with the configuration to build a new model.\n","        `num_choices`: the number of classes for the classifier. Default = 2.\n","\n","    Inputs:\n","        `input_ids`: a torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n","            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n","            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n","        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n","            with the token types indices selected in [0, 1]. Type 0 corresponds to a `sentence A`\n","            and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n","        `attention_mask`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length] with indices\n","            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n","            input sequence length in the current batch. It's the mask that we typically use for attention when\n","            a batch has varying length sentences.\n","        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n","            with indices selected in [0, ..., num_choices].\n","\n","    Outputs:\n","        if `labels` is not `None`:\n","            Outputs the CrossEntropy classification loss of the output with the labels.\n","        if `labels` is `None`:\n","            Outputs the classification logits of shape [batch_size, num_labels].\n","\n","    Example usage:\n","    ```python\n","    # Already been converted into WordPiece token ids\n","    input_ids = torch.LongTensor([[[31, 51, 99], [15, 5, 0]], [[12, 16, 42], [14, 28, 57]]])\n","    input_mask = torch.LongTensor([[[1, 1, 1], [1, 1, 0]],[[1,1,0], [1, 0, 0]]])\n","    token_type_ids = torch.LongTensor([[[0, 0, 1], [0, 1, 0]],[[0, 1, 1], [0, 0, 1]]])\n","    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n","        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n","\n","    num_choices = 2\n","\n","    model = BertForMultipleChoice(config, num_choices)\n","    logits = model(input_ids, token_type_ids, input_mask)\n","    ```\n","    \"\"\"\n","    def __init__(self, config, num_choices):\n","        super(BertForMultipleChoice, self).__init__(config)\n","        self.num_choices = num_choices\n","        self.bert = BertModel(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, 1)\n","        self.apply(self.init_bert_weights)\n","\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n","        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n","        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n","        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n","        _, pooled_output = self.bert(flat_input_ids, flat_token_type_ids, flat_attention_mask, output_all_encoded_layers=False)\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","        reshaped_logits = logits.view(-1, self.num_choices)\n","\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss()\n","            loss = loss_fct(reshaped_logits, labels)\n","            return loss\n","        else:\n","            return reshaped_logits\n","\n","\n","class BertForTokenClassification(BertPreTrainedModel):\n","    \"\"\"BERT model for token-level classification.\n","    This module is composed of the BERT model with a linear layer on top of\n","    the full hidden state of the last layer.\n","\n","    Params:\n","        `config`: a BertConfig class instance with the configuration to build a new model.\n","        `num_labels`: the number of classes for the classifier. Default = 2.\n","\n","    Inputs:\n","        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n","            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n","            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n","        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n","            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n","            a `sentence B` token (see BERT paper for more details).\n","        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n","            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n","            input sequence length in the current batch. It's the mask that we typically use for attention when\n","            a batch has varying length sentences.\n","        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size, sequence_length]\n","            with indices selected in [0, ..., num_labels].\n","\n","    Outputs:\n","        if `labels` is not `None`:\n","            Outputs the CrossEntropy classification loss of the output with the labels.\n","        if `labels` is `None`:\n","            Outputs the classification logits of shape [batch_size, sequence_length, num_labels].\n","\n","    Example usage:\n","    ```python\n","    # Already been converted into WordPiece token ids\n","    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n","    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n","    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n","\n","    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n","        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n","\n","    num_labels = 2\n","\n","    model = BertForTokenClassification(config, num_labels)\n","    logits = model(input_ids, token_type_ids, input_mask)\n","    ```\n","    \"\"\"\n","    def __init__(self, config, num_labels):\n","        super(BertForTokenClassification, self).__init__(config)\n","        self.num_labels = num_labels\n","        self.bert = BertModel(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, num_labels)\n","        self.apply(self.init_bert_weights)\n","\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n","        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n","        sequence_output = self.dropout(sequence_output)\n","        logits = self.classifier(sequence_output)\n","\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss()\n","            # Only keep active parts of the loss\n","            if attention_mask is not None:\n","                active_loss = attention_mask.view(-1) == 1\n","                active_logits = logits.view(-1, self.num_labels)[active_loss]\n","                active_labels = labels.view(-1)[active_loss]\n","                loss = loss_fct(active_logits, active_labels)\n","            else:\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            return loss\n","        else:\n","            return logits\n","\n","\n","class BertForQuestionAnswering(BertPreTrainedModel):\n","    \"\"\"BERT model for Question Answering (span extraction).\n","    This module is composed of the BERT model with a linear layer on top of\n","    the sequence output that computes start_logits and end_logits\n","\n","    Params:\n","        `config`: a BertConfig class instance with the configuration to build a new model.\n","\n","    Inputs:\n","        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n","            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n","            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n","        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n","            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n","            a `sentence B` token (see BERT paper for more details).\n","        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n","            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n","            input sequence length in the current batch. It's the mask that we typically use for attention when\n","            a batch has varying length sentences.\n","        `start_positions`: position of the first token for the labeled span: torch.LongTensor of shape [batch_size].\n","            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n","            into account for computing the loss.\n","        `end_positions`: position of the last token for the labeled span: torch.LongTensor of shape [batch_size].\n","            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n","            into account for computing the loss.\n","\n","    Outputs:\n","        if `start_positions` and `end_positions` are not `None`:\n","            Outputs the total_loss which is the sum of the CrossEntropy loss for the start and end token positions.\n","        if `start_positions` or `end_positions` is `None`:\n","            Outputs a tuple of start_logits, end_logits which are the logits respectively for the start and end\n","            position tokens of shape [batch_size, sequence_length].\n","\n","    Example usage:\n","    ```python\n","    # Already been converted into WordPiece token ids\n","    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n","    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n","    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n","\n","    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n","        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n","\n","    model = BertForQuestionAnswering(config)\n","    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\n","    ```\n","    \"\"\"\n","    def __init__(self, config):\n","        super(BertForQuestionAnswering, self).__init__(config)\n","        self.bert = BertModel(config)\n","        # TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version\n","        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n","        self.apply(self.init_bert_weights)\n","\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None, end_positions=None):\n","        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n","        logits = self.qa_outputs(sequence_output)\n","        start_logits, end_logits = logits.split(1, dim=-1)\n","        start_logits = start_logits.squeeze(-1)\n","        end_logits = end_logits.squeeze(-1)\n","\n","        if start_positions is not None and end_positions is not None:\n","            # If we are on multi-GPU, split add a dimension\n","            if len(start_positions.size()) > 1:\n","                start_positions = start_positions.squeeze(-1)\n","            if len(end_positions.size()) > 1:\n","                end_positions = end_positions.squeeze(-1)\n","            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n","            ignored_index = start_logits.size(1)\n","            start_positions.clamp_(0, ignored_index)\n","            end_positions.clamp_(0, ignored_index)\n","\n","            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n","            start_loss = loss_fct(start_logits, start_positions)\n","            end_loss = loss_fct(end_logits, end_positions)\n","            total_loss = (start_loss + end_loss) / 2\n","            return total_loss\n","        else:\n","            return start_logits, end_logits\n"],"metadata":{"id":"WG1xqrsAgIzW","executionInfo":{"status":"ok","timestamp":1669474199118,"user_tz":-480,"elapsed":694,"user":{"displayName":"胡博程","userId":"10006746442944693195"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["bert model模块"],"metadata":{"id":"3WgMe8Y_gvsU"}},{"cell_type":"code","source":["# coding: UTF-8\n","import torch\n","import torch.nn as nn\n","# from pytorch_pretrained_bert import BertModel, BertTokenizer\n","# from pytorch_pretrained import BertModel, BertTokenizer\n","\n","\n","class Config(object):\n","\n","    \"\"\"配置参数\"\"\"\n","    def __init__(self, dataset):\n","        self.model_name = 'bert'\n","        #self.train_path = dataset + '/data/train.txt'                                # 训练集\n","        #self.dev_path = dataset + '/data/dev.txt'                                    # 验证集\n","        #self.test_path = dataset + '/data/test.txt'                                  # 测试集\n","        self.class_list = [x.strip() for x in open(\n","            '/content/drive/MyDrive/case2/THUCNews/data/class.txt', encoding='utf-8').readlines()]                                # 类别名单\n","        self.save_path = '/content/drive/MyDrive/case2/THUCNews/saved_dict/bert.ckpt'        # 模型训练结果\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备\n","\n","        self.require_improvement = 1000                                 # 若超过1000batch效果还没提升，则提前结束训练\n","        self.num_classes = len(self.class_list)                         # 类别数\n","        self.num_epochs = 12                                             # epoch数\n","        self.batch_size = 128                                            # mini-batch大小\n","        self.pad_size = 80                                              # 每句话处理成的长度(短填长切)\n","        self.learning_rate = 5e-5                                       # 学习率\n","        self.bert_path = '/content/drive/MyDrive/case2/bert_pretrain'\n","        self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)\n","        self.hidden_size = 768\n","\n","\n","class Model(nn.Module):\n","\n","    def __init__(self, config):\n","        super(Model, self).__init__()\n","        self.bert = BertModel.from_pretrained(config.bert_path)\n","        for param in self.bert.parameters():\n","            param.requires_grad = True\n","        self.fc = nn.Linear(config.hidden_size, config.num_classes)\n","\n","    def forward(self, x):\n","        context = x[0]  # 输入的句子\n","        mask = x[2]  # 对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]\n","        _, pooled = self.bert(context, attention_mask=mask, output_all_encoded_layers=False)\n","        out = self.fc(pooled)\n","        return out\n"],"metadata":{"id":"pOSkiZY7g0FB","executionInfo":{"status":"ok","timestamp":1669477780559,"user_tz":-480,"elapsed":995,"user":{"displayName":"胡博程","userId":"10006746442944693195"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["file_ulity.py\n"],"metadata":{"id":"b8Ul0G0elo_F"}},{"cell_type":"code","source":["!pip install boto3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AYwXwvtDl4xk","executionInfo":{"status":"ok","timestamp":1669474215109,"user_tz":-480,"elapsed":3011,"user":{"displayName":"胡博程","userId":"10006746442944693195"}},"outputId":"8346b570-06ba-45a8-a6bc-c8559a5f223a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (1.26.16)\n","Requirement already satisfied: botocore<1.30.0,>=1.29.16 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.29.16)\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.6.0)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.0.1)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.16->boto3) (2.8.2)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.16->boto3) (1.26.13)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.16->boto3) (1.15.0)\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Utilities for working with the local dataset cache.\n","This file is adapted from the AllenNLP library at https://github.com/allenai/allennlp\n","Copyright by the AllenNLP authors.\n","\"\"\"\n","from __future__ import (absolute_import, division, print_function, unicode_literals)\n","\n","import sys\n","import json\n","import logging\n","import os\n","import shutil\n","import tempfile\n","import fnmatch\n","from functools import wraps\n","from hashlib import sha256\n","import sys\n","from io import open\n","\n","import boto3\n","import requests\n","from botocore.exceptions import ClientError\n","from tqdm import tqdm\n","\n","try:\n","    from urllib.parse import urlparse\n","except ImportError:\n","    from urlparse import urlparse\n","\n","try:\n","    from pathlib import Path\n","    PYTORCH_PRETRAINED_BERT_CACHE = Path(os.getenv('PYTORCH_PRETRAINED_BERT_CACHE',\n","                                                   Path.home() / '.pytorch_pretrained_bert'))\n","except (AttributeError, ImportError):\n","    PYTORCH_PRETRAINED_BERT_CACHE = os.getenv('PYTORCH_PRETRAINED_BERT_CACHE',\n","                                              os.path.join(os.path.expanduser(\"~\"), '.pytorch_pretrained_bert'))\n","\n","CONFIG_NAME = \"config.json\"\n","WEIGHTS_NAME = \"pytorch_model.bin\"\n","\n","logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n","\n","\n","def url_to_filename(url, etag=None):\n","    \"\"\"\n","    Convert `url` into a hashed filename in a repeatable way.\n","    If `etag` is specified, append its hash to the url's, delimited\n","    by a period.\n","    \"\"\"\n","    url_bytes = url.encode('utf-8')\n","    url_hash = sha256(url_bytes)\n","    filename = url_hash.hexdigest()\n","\n","    if etag:\n","        etag_bytes = etag.encode('utf-8')\n","        etag_hash = sha256(etag_bytes)\n","        filename += '.' + etag_hash.hexdigest()\n","\n","    return filename\n","\n","\n","def filename_to_url(filename, cache_dir=None):\n","    \"\"\"\n","    Return the url and etag (which may be ``None``) stored for `filename`.\n","    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n","    \"\"\"\n","    if cache_dir is None:\n","        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n","    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n","        cache_dir = str(cache_dir)\n","\n","    cache_path = os.path.join(cache_dir, filename)\n","    if not os.path.exists(cache_path):\n","        raise EnvironmentError(\"file {} not found\".format(cache_path))\n","\n","    meta_path = cache_path + '.json'\n","    if not os.path.exists(meta_path):\n","        raise EnvironmentError(\"file {} not found\".format(meta_path))\n","\n","    with open(meta_path, encoding=\"utf-8\") as meta_file:\n","        metadata = json.load(meta_file)\n","    url = metadata['url']\n","    etag = metadata['etag']\n","\n","    return url, etag\n","\n","\n","def cached_path(url_or_filename, cache_dir=None):\n","    \"\"\"\n","    Given something that might be a URL (or might be a local path),\n","    determine which. If it's a URL, download the file and cache it, and\n","    return the path to the cached file. If it's already a local path,\n","    make sure the file exists and then return the path.\n","    \"\"\"\n","    if cache_dir is None:\n","        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n","    if sys.version_info[0] == 3 and isinstance(url_or_filename, Path):\n","        url_or_filename = str(url_or_filename)\n","    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n","        cache_dir = str(cache_dir)\n","\n","    parsed = urlparse(url_or_filename)\n","\n","    if parsed.scheme in ('http', 'https', 's3'):\n","        # URL, so get it from the cache (downloading if necessary)\n","        return get_from_cache(url_or_filename, cache_dir)\n","    elif os.path.exists(url_or_filename):\n","        # File, and it exists.\n","        return url_or_filename\n","    elif parsed.scheme == '':\n","        # File, but it doesn't exist.\n","        raise EnvironmentError(\"file {} not found\".format(url_or_filename))\n","    else:\n","        # Something unknown\n","        raise ValueError(\"unable to parse {} as a URL or as a local path\".format(url_or_filename))\n","\n","\n","def split_s3_path(url):\n","    \"\"\"Split a full s3 path into the bucket name and path.\"\"\"\n","    parsed = urlparse(url)\n","    if not parsed.netloc or not parsed.path:\n","        raise ValueError(\"bad s3 path {}\".format(url))\n","    bucket_name = parsed.netloc\n","    s3_path = parsed.path\n","    # Remove '/' at beginning of path.\n","    if s3_path.startswith(\"/\"):\n","        s3_path = s3_path[1:]\n","    return bucket_name, s3_path\n","\n","\n","def s3_request(func):\n","    \"\"\"\n","    Wrapper function for s3 requests in order to create more helpful error\n","    messages.\n","    \"\"\"\n","\n","    @wraps(func)\n","    def wrapper(url, *args, **kwargs):\n","        try:\n","            return func(url, *args, **kwargs)\n","        except ClientError as exc:\n","            if int(exc.response[\"Error\"][\"Code\"]) == 404:\n","                raise EnvironmentError(\"file {} not found\".format(url))\n","            else:\n","                raise\n","\n","    return wrapper\n","\n","\n","@s3_request\n","def s3_etag(url):\n","    \"\"\"Check ETag on S3 object.\"\"\"\n","    s3_resource = boto3.resource(\"s3\")\n","    bucket_name, s3_path = split_s3_path(url)\n","    s3_object = s3_resource.Object(bucket_name, s3_path)\n","    return s3_object.e_tag\n","\n","\n","@s3_request\n","def s3_get(url, temp_file):\n","    \"\"\"Pull a file directly from S3.\"\"\"\n","    s3_resource = boto3.resource(\"s3\")\n","    bucket_name, s3_path = split_s3_path(url)\n","    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n","\n","\n","def http_get(url, temp_file):\n","    req = requests.get(url, stream=True)\n","    content_length = req.headers.get('Content-Length')\n","    total = int(content_length) if content_length is not None else None\n","    progress = tqdm(unit=\"B\", total=total)\n","    for chunk in req.iter_content(chunk_size=1024):\n","        if chunk: # filter out keep-alive new chunks\n","            progress.update(len(chunk))\n","            temp_file.write(chunk)\n","    progress.close()\n","\n","\n","def get_from_cache(url, cache_dir=None):\n","    \"\"\"\n","    Given a URL, look for the corresponding dataset in the local cache.\n","    If it's not there, download it. Then return the path to the cached file.\n","    \"\"\"\n","    if cache_dir is None:\n","        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n","    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n","        cache_dir = str(cache_dir)\n","\n","    if not os.path.exists(cache_dir):\n","        os.makedirs(cache_dir)\n","\n","    # Get eTag to add to filename, if it exists.\n","    if url.startswith(\"s3://\"):\n","        etag = s3_etag(url)\n","    else:\n","        try:\n","            response = requests.head(url, allow_redirects=True)\n","            if response.status_code != 200:\n","                etag = None\n","            else:\n","                etag = response.headers.get(\"ETag\")\n","        except EnvironmentError:\n","            etag = None\n","\n","    if sys.version_info[0] == 2 and etag is not None:\n","        etag = etag.decode('utf-8')\n","    filename = url_to_filename(url, etag)\n","\n","    # get cache path to put the file\n","    cache_path = os.path.join(cache_dir, filename)\n","\n","    # If we don't have a connection (etag is None) and can't identify the file\n","    # try to get the last downloaded one\n","    if not os.path.exists(cache_path) and etag is None:\n","        matching_files = fnmatch.filter(os.listdir(cache_dir), filename + '.*')\n","        matching_files = list(filter(lambda s: not s.endswith('.json'), matching_files))\n","        if matching_files:\n","            cache_path = os.path.join(cache_dir, matching_files[-1])\n","\n","    if not os.path.exists(cache_path):\n","        # Download to temporary file, then copy to cache dir once finished.\n","        # Otherwise you get corrupt cache entries if the download gets interrupted.\n","        with tempfile.NamedTemporaryFile() as temp_file:\n","            logger.info(\"%s not found in cache, downloading to %s\", url, temp_file.name)\n","\n","            # GET file object\n","            if url.startswith(\"s3://\"):\n","                s3_get(url, temp_file)\n","            else:\n","                http_get(url, temp_file)\n","\n","            # we are copying the file before closing it, so flush to avoid truncation\n","            temp_file.flush()\n","            # shutil.copyfileobj() starts at the current position, so go to the start\n","            temp_file.seek(0)\n","\n","            logger.info(\"copying %s to cache at %s\", temp_file.name, cache_path)\n","            with open(cache_path, 'wb') as cache_file:\n","                shutil.copyfileobj(temp_file, cache_file)\n","\n","            logger.info(\"creating metadata file for %s\", cache_path)\n","            meta = {'url': url, 'etag': etag}\n","            meta_path = cache_path + '.json'\n","            with open(meta_path, 'w') as meta_file:\n","                output_string = json.dumps(meta)\n","                if sys.version_info[0] == 2 and isinstance(output_string, str):\n","                    output_string = unicode(output_string, 'utf-8')  # The beauty of python 2\n","                meta_file.write(output_string)\n","\n","            logger.info(\"removing temp file %s\", temp_file.name)\n","\n","    return cache_path\n","\n","\n","def read_set_from_file(filename):\n","    '''\n","    Extract a de-duped collection (set) of text from a file.\n","    Expected file format is one item per line.\n","    '''\n","    collection = set()\n","    with open(filename, 'r', encoding='utf-8') as file_:\n","        for line in file_:\n","            collection.add(line.rstrip())\n","    return collection\n","\n","\n","def get_file_extension(path, dot=True, lower=True):\n","    ext = os.path.splitext(path)[1]\n","    ext = ext if dot else ext[1:]\n","    return ext.lower() if lower else ext\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HxAbuQBglolI","executionInfo":{"status":"ok","timestamp":1669474218709,"user_tz":-480,"elapsed":1003,"user":{"displayName":"胡博程","userId":"10006746442944693195"}},"outputId":"5c8c913d-aef5-4c72-d5cd-515dc2a6a271"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (3.0.4) doesn't match a supported version!\n","  RequestsDependencyWarning)\n"]}]},{"cell_type":"code","source":["\n","model_name = 'bert'  # bert\n","config = Config(dataset)\n","np.random.seed(1)\n","torch.manual_seed(1)\n","torch.cuda.manual_seed_all(1)\n","torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n","\n","start_time = time.time()\n","print(\"Loading data...\")\n","train_data, dev_data, test_data = build_dataset(x_train,y_train,x_dev,y_dev,x_test,y_test,config)\n","train_iter = build_iterator(train_data, config)\n","dev_iter = build_iterator(dev_data, config)\n","test_iter = build_iterator(test_data, config)\n","time_dif = get_time_dif(start_time)\n","print(\"Time usage:\", time_dif)"],"metadata":{"id":"q8jXCHn-U9om","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669477802080,"user_tz":-480,"elapsed":13899,"user":{"displayName":"胡博程","userId":"10006746442944693195"}},"outputId":"d4ff492d-c806-46da-924f-da6e5f805dbb"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data...\n","Time usage: 0:00:13\n"]}]},{"cell_type":"code","source":["# train\n","model = Model(config).to(config.device)\n","train(config, model, train_iter, dev_iter, test_iter)"],"metadata":{"id":"5sReNydcVQyd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669479210642,"user_tz":-480,"elapsed":1408572,"user":{"displayName":"胡博程","userId":"10006746442944693195"}},"outputId":"78e48f4e-1932-40ac-c2db-aaff44a4fa05"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/12]\n","Iter:      0,  Train Loss:   0.9,  Train Acc: 32.81%,  Val Loss:  0.96,  Val Acc: 27.38%,  Time: 0:00:11 *\n","Epoch [2/12]\n","Iter:    100,  Train Loss:  0.22,  Train Acc: 92.19%,  Val Loss:  0.26,  Val Acc: 89.75%,  Time: 0:02:53 *\n","Epoch [3/12]\n","Iter:    200,  Train Loss: 0.016,  Train Acc: 100.00%,  Val Loss:  0.14,  Val Acc: 95.14%,  Time: 0:05:40 *\n","Epoch [4/12]\n","Epoch [5/12]\n","Iter:    300,  Train Loss: 0.045,  Train Acc: 98.44%,  Val Loss:  0.15,  Val Acc: 95.66%,  Time: 0:08:30 \n","Epoch [6/12]\n","Iter:    400,  Train Loss: 0.018,  Train Acc: 100.00%,  Val Loss:  0.13,  Val Acc: 96.69%,  Time: 0:11:21 *\n","Epoch [7/12]\n","Epoch [8/12]\n","Iter:    500,  Train Loss: 0.0063,  Train Acc: 100.00%,  Val Loss:  0.12,  Val Acc: 97.17%,  Time: 0:14:11 *\n","Epoch [9/12]\n","Iter:    600,  Train Loss: 0.038,  Train Acc: 97.66%,  Val Loss:  0.13,  Val Acc: 96.98%,  Time: 0:17:01 \n","Epoch [10/12]\n","Epoch [11/12]\n","Iter:    700,  Train Loss: 0.0046,  Train Acc: 100.00%,  Val Loss:  0.13,  Val Acc: 97.03%,  Time: 0:19:49 \n","Epoch [12/12]\n","Iter:    800,  Train Loss: 0.0017,  Train Acc: 100.00%,  Val Loss:  0.14,  Val Acc: 97.12%,  Time: 0:22:39 \n","Test Loss:  0.47,  Test Acc: 91.46%\n","Precision, Recall and F1-Score...\n","              precision    recall  f1-score   support\n","\n","         真新闻     0.9349    0.9673    0.9508      8659\n","         假新闻     0.7606    0.6066    0.6749      1482\n","\n","    accuracy                         0.9146     10141\n","   macro avg     0.8478    0.7870    0.8129     10141\n","weighted avg     0.9094    0.9146    0.9105     10141\n","\n","Confusion Matrix...\n","[[8376  283]\n"," [ 583  899]]\n","Time usage: 0:00:44\n"]}]},{"cell_type":"code","source":["# memory footprint support libraries/code\n","!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n","!pip install gputil\n","!pip install psutil\n","!pip install humanize\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l-CFZLhCYCMi","executionInfo":{"status":"ok","timestamp":1669473638538,"user_tz":-480,"elapsed":10808,"user":{"displayName":"胡博程","userId":"10006746442944693195"}},"outputId":"d0f1ff5c-342f-4c51-a41a-08148a9b6af9"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gputil\n","  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n","Building wheels for collected packages: gputil\n","  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7409 sha256=547840b2c4a4fd7cf1e4a8a2fded8a7724f735b053155c0fc29220d5564a9ff4\n","  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n","Successfully built gputil\n","Installing collected packages: gputil\n","Successfully installed gputil-1.4.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n"]}]},{"cell_type":"code","source":["import psutil\n","import humanize\n","import os\n","import GPUtil as GPU\n","\n","GPUs = GPU.getGPUs()\n","# XXX: only one GPU on Colab and isn’t guaranteed\n","gpu = GPUs[0]\n","def printm():\n","    process = psutil.Process(os.getpid())\n","    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n","    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n","printm()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eobtufQEYEgq","executionInfo":{"status":"ok","timestamp":1669474115408,"user_tz":-480,"elapsed":530,"user":{"displayName":"胡博程","userId":"10006746442944693195"}},"outputId":"7e94ae41-6013-4b32-f842-7f71603d7462"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Gen RAM Free: 10.2 GB  |     Proc size: 3.9 GB\n","GPU RAM Free: 15MB | Used: 15094MB | Util 100% | Total     15109MB\n"]}]},{"cell_type":"code","source":["!ps -aux|grep python\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PSvuAudfYIAe","executionInfo":{"status":"ok","timestamp":1669474119724,"user_tz":-480,"elapsed":479,"user":{"displayName":"胡博程","userId":"10006746442944693195"}},"outputId":"307948c9-90b0-4a57-8b61-f96b90bc3de8"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["root          35  0.1  0.0      0     0 ?        Z    14:12   0:04 [python3] <defunct>\n","root          36  0.0  0.3 162272 43284 ?        S    14:12   0:00 python3 /usr/local/bin/colab-fileshim.py\n","root          63  0.1  0.4 199460 61028 ?        Sl   14:12   0:03 /usr/bin/python3 /usr/local/bin/jupyter-notebook --ip=172.28.0.2 --port=9000 --FileContentsManager.root_dir=/ --MappingKernelManager.root_dir=/content\n","root         277  0.0  0.0  18388  3044 ?        S    14:12   0:00 bash -c tail -n +0 -F \"/root/.config/Google/DriveFS/Logs/drive_fs.txt\" | python3 /opt/google/drive/drive-filter.py > \"/root/.config/Google/DriveFS/Logs/timeouts.txt\" \n","root         280  0.0  0.0  31300  9508 ?        S    14:12   0:00 python3 /opt/google/drive/drive-filter.py\n","root         763 13.3 28.4 27214640 3783968 ?    Ssl  14:42   0:49 /usr/bin/python3 -m ipykernel_launcher -f /root/.local/share/jupyter/runtime/kernel-8b9c7464-14e7-487d-bf88-0305f91c7713.json\n","root         779  0.2  0.1 127964 16516 ?        Sl   14:42   0:00 /usr/bin/python3 /usr/local/lib/python3.7/dist-packages/debugpy/adapter --for-server 38515 --host 127.0.0.1 --port 20650 --server-access-token 76cf311d1fc607fb0130027e6ee7e164f1c934706bfd602d494956ab0286e011\n","root         908  0.0  0.0  39208  6512 ?        S    14:48   0:00 /bin/bash -c ps -aux|grep python\n","root         910  0.0  0.0  38576  5448 ?        S    14:48   0:00 grep python\n"]}]},{"cell_type":"code","source":["!kill -9 763"],"metadata":{"id":"JeotZCATYxdM"},"execution_count":null,"outputs":[]}]}